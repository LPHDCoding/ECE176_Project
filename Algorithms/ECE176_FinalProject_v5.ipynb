{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 176: Fine-Grained Classification Using a CB-ViT Model\n",
    "\n",
    "The focus of our final project will be reimplmenenting the work of Shuo Zhu, Xukang Zhang, Yu Wang, Zhongyang Wang, and Jiahao Sun. The main result of this paper is the introduction of a CB-ViT model. This model combines the local feature extraction of Convolutional networks with the broad feature extraction of Vision Transformers. A version of their reseach paper can be found here: https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.13295. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - reused from assignment 5\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setup - similar to assignment 5\n",
    "\n",
    "train_transform = transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((448, 448))\n",
    "    ])\n",
    "\n",
    "# Oxford 102 dataset for train, val, and test.\n",
    "flower_train = dset.Flowers102(\"./datasets/flowers\", split='test', download=True, transform=train_transform) # The 'test' split has 6k+ images\n",
    "flower_val   = dset.Flowers102(\"./datasets/flowers\", split='val', download=True, transform=transform)\n",
    "flower_test  = dset.Flowers102(\"./datasets/flowers\", split='train', download=True, transform=transform) # The 'train' split has 1k images\n",
    "\n",
    "NUM_TRAIN = 6000\n",
    "NUM_VAL = 1000\n",
    "NUM_TEST = 1000\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    flower_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "loader_val = DataLoader(\n",
    "    flower_val,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    flower_test,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Dtype and device selection - reused from assignment 5\n",
    "\n",
    "USE_GPU = True\n",
    "num_class = 100\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten function - reused from assignment 5\n",
    "\n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5847, -0.5212,  1.3914,  0.6470, -0.1718],\n",
       "        [ 1.4946, -0.1659, -0.2562, -0.8890, -0.7908],\n",
       "        [ 0.6552,  0.4206,  0.1405, -0.7177, -0.7956]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random weight function - reused from assignment 5\n",
    "\n",
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# create a weight of shape [3 x 5]\n",
    "# you should see the type `torch.cuda.FloatTensor` if you use GPU. \n",
    "# Otherwise it should be `torch.FloatTensor`\n",
    "random_weight((3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy function - reused from assignment 5\n",
    "\n",
    "def check_accuracy_part34(loader, model, print_flag=True):\n",
    "    \"\"\"if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\"\"\"   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        if(print_flag==True):\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function - reused from assignment 5\n",
    "\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: The accuracy of the model\n",
    "    \"\"\"\n",
    "    iterations = []\n",
    "    accuracies = []\n",
    "    itera = 1\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t + 1, loss.item()))\n",
    "                acc = check_accuracy_part34(loader_val, model)\n",
    "                print()\n",
    "                iterations.append(itera)\n",
    "                accuracies.append(acc)\n",
    "            itera = itera + 1\n",
    "        plt.plot(iterations, accuracies)\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "    return check_accuracy_part34(loader_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, F1, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(F1, F2, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size // 2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(F2, F3, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "\n",
    "        self.relu = nn.ReLU()   #self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        F1, F2, F3 = filters\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, F1, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(F1, F2, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size // 2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(F2, F3, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(F3)\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, F3, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(F3)\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU() #self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=102):\n",
    "        super(ResNet50, self).__init__()\n",
    "        # Initial convolution and max-pooling\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()   #self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Stage 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConvolutionalBlock(64, filters=(64, 64, 256), kernel_size=3, stride=1),\n",
    "            IdentityBlock(256, filters=(64, 64, 256), kernel_size=3),\n",
    "            IdentityBlock(256, filters=(64, 64, 256), kernel_size=3)\n",
    "        )\n",
    "\n",
    "        # Stage 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ConvolutionalBlock(256, filters=(128, 128, 512), kernel_size=3, stride=2),\n",
    "            IdentityBlock(512, filters=(128, 128, 512), kernel_size=3),\n",
    "            IdentityBlock(512, filters=(128, 128, 512), kernel_size=3),\n",
    "            IdentityBlock(512, filters=(128, 128, 512), kernel_size=3)\n",
    "        )\n",
    "\n",
    "        # Stage 4\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ConvolutionalBlock(512, filters=(256, 256, 1024), kernel_size=3, stride=2),\n",
    "            IdentityBlock(1024, filters=(256, 256, 1024), kernel_size=3),\n",
    "            IdentityBlock(1024, filters=(256, 256, 1024), kernel_size=3),\n",
    "            IdentityBlock(1024, filters=(256, 256, 1024), kernel_size=3),\n",
    "            IdentityBlock(1024, filters=(256, 256, 1024), kernel_size=3),\n",
    "            IdentityBlock(1024, filters=(256, 256, 1024), kernel_size=3)\n",
    "        )\n",
    "\n",
    "        # Stage 5\n",
    "        self.layer5 = nn.Sequential(\n",
    "            ConvolutionalBlock(1024, filters=(512, 512, 2048), kernel_size=3, stride=2),\n",
    "            IdentityBlock(2048, filters=(512, 512, 2048), kernel_size=3),\n",
    "            IdentityBlock(2048, filters=(512, 512, 2048), kernel_size=3)\n",
    "        )\n",
    "\n",
    "        # Average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 3, H, W)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # Given an input size of (1,3,448,448), the size of x after this line is [1, 64, 112, 112]\n",
    "\n",
    "        x = self.layer2(x)  # Given an input size of (1,3,448,448), the size of x after this line is [1, 256, 112, 112]\n",
    "        x = self.layer3(x)  # Given an input size of (1,3,448,448), the size of x after this line is [1, 512, 56, 56]\n",
    "        x = self.layer4(x)  # Given an input size of (1,3,448,448), the size of x after this line is [1, 1024, 28, 28]\n",
    "        x = self.layer5(x)  # Given an input size of (1,3,448,448), the size of x after this line is [1, 2048, 14, 14]\n",
    "\n",
    "        #x = self.avgpool(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"CNN Modules\"\n",
    "\n",
    "class FEM(nn.Module):\n",
    "    def __init__(self, in_channel, in_height, in_width):\n",
    "        super().__init__()\n",
    "        self.channel_half = int(0.5*(in_channel))\n",
    "        self.conv3_1 = nn.Conv2d(in_channel, self.channel_half, (3,3), padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3_1.weight)\n",
    "        self.batch31 = nn.BatchNorm2d(self.channel_half)\n",
    "        self.silu31 = nn.SiLU()\n",
    "        self.conv5 = nn.Conv2d(in_channel, self.channel_half, (5,5), padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        self.batch5 = nn.BatchNorm2d(self.channel_half)\n",
    "        self.silu5 = nn.SiLU()\n",
    "        self.max = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.avg = nn.AvgPool2d(kernel_size=(2,2))\n",
    "        self.conv3_2 = nn.Conv2d(self.channel_half, self.channel_half, (3,3), padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3_2.weight)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc_size = int(self.channel_half*(2*np.floor(in_height/2))*(np.floor(in_width/2)))\n",
    "        self.fc = nn.Linear(self.fc_size, in_width)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.in_channel = in_channel\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "    def forward(self, x):\n",
    "        R1 = self.silu31(self.batch31(self.conv3_1(x)))\n",
    "        R2 = self.silu5(self.batch5(self.conv5(x)))\n",
    "        Rm = R1 + R2\n",
    "        Rn = torch.cat((self.max(Rm), self.avg(Rm)), dim=2)\n",
    "        Rp = self.conv3_2(Rn)\n",
    "        M = self.fc(self.flatten(Rp))\n",
    "        gamma = self.soft(M)\n",
    "        batch_size = x.shape[0]\n",
    "        gamma_expand = gamma.view(batch_size,1,1,self.in_width)\n",
    "        Rx_1 = torch.mul(R1, gamma_expand)\n",
    "        Rx_2 = torch.mul(R2, 1-gamma_expand)\n",
    "        Rx = Rx_1 + Rx_2\n",
    "        return Rx\n",
    "    \n",
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self, in_channel, in_height, in_width):\n",
    "        super().__init__()\n",
    "        self.channel_half = int(in_channel/2)\n",
    "        self.channel_quarter = int(self.channel_half/2)\n",
    "        self.conv1 = nn.Conv2d(in_channel, self.channel_half, (1,1), padding=0)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        self.batch1 = nn.BatchNorm2d(self.channel_half)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.conv3 = nn.Conv2d(self.channel_half, self.channel_quarter, (3,3), padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        self.batch3 = nn.BatchNorm2d(self.channel_quarter)\n",
    "        self.silu3 = nn.SiLU()\n",
    "        self.FEM = FEM(self.channel_quarter, in_height, in_width)\n",
    "    def forward(self, x):\n",
    "        output = self.FEM(self.silu3(self.batch3(self.conv3(self.silu1(self.batch1(self.conv1(x)))))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFE \n",
    "\n",
    "class SFE(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        self.conv3_1 = nn.Conv2d(in_channel, in_channel, (3,3), stride=2, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv3_1.weight)\n",
    "        self.batch3_1 = nn.BatchNorm2d(in_channel)\n",
    "        self.relu3_1 = nn.ReLU()\n",
    "        self.conv3_2 = nn.Conv2d(in_channel, in_channel, (3,3), stride=2, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv3_2.weight)\n",
    "        self.batch3_2 = nn.BatchNorm2d(in_channel)\n",
    "        self.relu3_2 = nn.ReLU()\n",
    "        self.conv3_3 = nn.Conv2d(in_channel, in_channel, (3,3), stride=2, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv3_3.weight)\n",
    "        self.batch3_3 = nn.BatchNorm2d(in_channel)\n",
    "        self.relu3_3 = nn.ReLU()\n",
    "        self.conv3_4 = nn.Conv2d(in_channel, in_channel, (3,3), stride=2, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv3_4.weight)\n",
    "        self.batch3_4 = nn.BatchNorm2d(in_channel)\n",
    "        self.relu3_4 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channel, in_channel, (1,1), stride=1, padding=3) # Change padding to 3 so that 448x448 input becomes 36x36 output\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        self.batch1 = nn.BatchNorm2d(in_channel)\n",
    "        self.relu1 = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        output = self.relu1(self.batch1(self.conv1(self.relu3_4(self.batch3_4(self.conv3_4(self.relu3_3(self.batch3_3(self.conv3_3(self.relu3_2(self.batch3_2(self.conv3_2(self.relu3_1(self.batch3_1(self.conv3_1(x)))))))))))))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ViT Modules\"\n",
    "\n",
    "class PatchEmbedding(nn.Module):   # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, in_channels=3, patch_size=12, emb_size=48):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.projection(x)\n",
    "        return output\n",
    "\n",
    "class Attention(nn.Module): # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=dropout)\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attention_out, attention_out_weights = self.attention(q, k, v)\n",
    "        return attention_out\n",
    "    \n",
    "class PreNorm(nn.Module):   # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, dim, function):\n",
    "        super().__init__()\n",
    "        self.layer = nn.LayerNorm(dim)\n",
    "        self.function = function\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.function(self.layer(x), **kwargs)\n",
    "    \n",
    "class FeedForward(nn.Sequential):   # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, dim, hidden_dim, dropout=0):\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "class ResidualAdd(nn.Module): # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, function):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        output = self.function(x, **kwargs)\n",
    "        output += res\n",
    "        return output\n",
    "    \n",
    "class ViT(nn.Module): # Obtained from tutorial: https://youtu.be/j3VNqtJUoz0?si=iZLnmtbygLGLQl9K\n",
    "    def __init__(self, ch=3, img_size=36, patch_size=12, emb_dim=432,   #Not sure about emb_dim since each patch is 3x12x12 (CxHxW) = 432 but that is big\n",
    "                n_layers=12, out_dim=102, dropout=0.1, heads=4):\n",
    "        super(ViT, self).__init__()\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=ch, patch_size=patch_size, emb_size=emb_dim)\n",
    "        # Learnable params\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
    "    def forward(self, img, cnn_in):\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        # Add cls token to inputs\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = x + cnn_in\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "        # Output based on classification token\n",
    "        return self.head(x[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CB_ViT Model\n",
    "\n",
    "class CB_ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ResNet = ResNet50()    # For (1,3,448,448) input, expecting [1, 2048, 14, 14] output\n",
    "        self.CNN_modules = CNN_Block(in_channel=2048, in_height=14, in_width=14)\n",
    "        self.SFE = SFE(in_channel=3)\n",
    "        self.conv1_down = nn.Conv2d(in_channels=256, out_channels=10, kernel_size=(1,1))\n",
    "        self.adapt = nn.AdaptiveAvgPool2d(output_size=(432, 1))\n",
    "        self.layer = nn.LayerNorm(432)  # em_dim = 432\n",
    "        self.ViT = ViT(ch=3, img_size=36, patch_size=12, emb_dim=432, n_layers=12, out_dim=102, dropout=0.1, heads=4)\n",
    "    def forward(self, x):\n",
    "        # CNN branch\n",
    "        resnet50_out = self.ResNet(x)\n",
    "        cnn_modules_out = self.CNN_modules(resnet50_out)    # For [1, 2048, 14, 14] ResNet output, expecting [1, 256, 14, 14] output, needs to be converted to [1, 10, 432]\n",
    "        # Down converter\n",
    "        down_out = self.conv1_down(cnn_modules_out) # For [1, 256, 14, 14] input, expecting [1, 10, 14, 14] output\n",
    "        down_adapt = self.adapt(down_out)   # For [1, 10, 14, 14] input, obtain [1, 10, 432, 1] output\n",
    "        down_resize = down_adapt.view(down_adapt.shape[0], down_adapt.shape[1], -1)\n",
    "        down_layer = self.layer(down_resize)\n",
    "        # ViT branch    \n",
    "        sfe_out = self.SFE(x)\n",
    "        output = self.ViT(img=sfe_out, cnn_in=down_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet_ViT Model\n",
    "\n",
    "class Res_ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ResNet = ResNet50()\n",
    "        self.y = torch.zeros((8,5,1024), device=device, dtype=dtype) # Batch_size, num_patches + 1, emb_dim\n",
    "        self.ViT = ViT(ch=2048, img_size=14, patch_size=7, emb_dim=1024, n_layers=12, out_dim=102, dropout=0.1, heads=4)\n",
    "    def forward(self, x):\n",
    "        resnet_out = self.ResNet(x)\n",
    "        output = self.ViT(img=resnet_out, cnn_in=self.y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Res_ViT\n",
    "\"\"\"x = torch.ones((8,3,448,448), dtype=dtype) # NxCxHxW\n",
    "model = Res_ViT()\n",
    "output = model(x)\n",
    "print(output.size())\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 100, loss = 4.4619\n",
      "Got 10 / 1020 correct (0.98)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Res_ViT\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00005\n",
    "epochs = 30#0\n",
    "\n",
    "model = CB_ViT()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "print_every = 100\n",
    "train_part34(model, optimizer, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE176",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
